---
phase: 01-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/__init__.py
  - src/__main__.py
  - src/config.py
  - tests/__init__.py
  - tests/conftest.py
  - tests/test_config.py
  - tests/fixtures/chat_completion.json
  - tests/fixtures/tool_call_response.json
  - tests/fixtures/content_filter_error.json
  - requirements.txt
  - requirements-dev.txt
  - pyproject.toml
autonomous: true
requirements: []

must_haves:
  truths:
    - "Running python -m src.config loads env vars from .env and displays a pass/fail summary table"
    - "Config validation shows ALL missing env vars at once (not fail-fast)"
    - "Connectivity checks are skipped if required env vars are missing"
    - "Content filter errors produce a specific actionable message, not a generic error"
    - "pytest discovers and runs tests from the tests/ directory"
    - "Mock fixtures enable development without live Azure resources"
  artifacts:
    - path: "src/__init__.py"
      provides: "Package init for src module"
    - path: "src/__main__.py"
      provides: "Entry point for python -m src.config execution"
      contains: "config"
    - path: "src/config.py"
      provides: "Settings dataclass, env validation, connectivity checks, summary table"
      exports: ["Settings", "load_settings", "validate_env_vars", "test_openai_connectivity", "test_sentinel_connectivity"]
      min_lines: 80
    - path: "tests/__init__.py"
      provides: "Package init for tests module"
    - path: "tests/conftest.py"
      provides: "Shared pytest fixtures loading JSON mock data"
      contains: "fixtures"
    - path: "tests/test_config.py"
      provides: "Unit tests for config validation and error handling"
      min_lines: 40
    - path: "tests/fixtures/chat_completion.json"
      provides: "Mock successful chat completion response"
      contains: "chat.completion"
    - path: "tests/fixtures/tool_call_response.json"
      provides: "Mock tool call response with function arguments"
      contains: "tool_calls"
    - path: "tests/fixtures/content_filter_error.json"
      provides: "Mock content filter rejection response"
      contains: "content_filter"
    - path: "requirements.txt"
      provides: "Pinned production dependencies for Phase 1"
      contains: "openai"
    - path: "requirements-dev.txt"
      provides: "Pinned development dependencies"
      contains: "pytest"
    - path: "pyproject.toml"
      provides: "Project config with Python 3.12 target, ruff, pytest settings"
      contains: "requires-python"
  key_links:
    - from: "src/__main__.py"
      to: "src/config.py"
      via: "imports and calls main() or validate_and_display()"
      pattern: "from.*config.*import"
    - from: "src/config.py"
      to: ".env"
      via: "python-dotenv load_dotenv()"
      pattern: "load_dotenv"
    - from: "src/config.py"
      to: "openai.AzureOpenAI"
      via: "connectivity check using chat.completions.create"
      pattern: "AzureOpenAI"
    - from: "src/config.py"
      to: "azure.monitor.query.LogsQueryClient"
      via: "connectivity check using query_workspace"
      pattern: "LogsQueryClient"
    - from: "src/config.py"
      to: "rich.table.Table"
      via: "summary table display"
      pattern: "Table"
    - from: "tests/test_config.py"
      to: "src/config.py"
      via: "imports functions under test"
      pattern: "from src.config import"
    - from: "tests/conftest.py"
      to: "tests/fixtures/"
      via: "loads JSON files with pathlib + json.load"
      pattern: "fixtures"
---

<objective>
Scaffold the Python project structure and implement the configuration module with layered validation (env vars first, then connectivity), content-filter-specific error detection, mock Azure OpenAI fixtures, and a comprehensive test suite.

Purpose: Establish the codebase foundation that all future phases build on, with a working config validation command and test infrastructure.
Output: Complete src/ and tests/ packages, dependency files, project config, passing test suite
</objective>

<execution_context>
@C:/Users/AlexSandstrom/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/AlexSandstrom/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project skeleton and dependency files</name>
  <files>src/__init__.py, tests/__init__.py, requirements.txt, requirements-dev.txt, pyproject.toml</files>
  <action>
Create the project skeleton with proper Python package structure.

**src/__init__.py:** Empty file (package init).

**tests/__init__.py:** Empty file (package init).

**requirements.txt** -- pinned production dependencies (Phase 1 only):
```
azure-identity>=1.25.0
azure-monitor-query>=2.0.0
openai>=2.21.0
python-dotenv>=1.0.0
rich>=14.0.0
```

**requirements-dev.txt** -- pinned dev dependencies:
```
-r requirements.txt
pytest>=9.0.0
ruff>=0.15.0
```

**pyproject.toml** -- use the exact template from 01-RESEARCH.md:
- `name = "sentinel-rag-chatbot"`, `version = "0.1.0"`
- `requires-python = ">=3.11,<3.14"` (avoid Python 3.14 per blocker)
- `[tool.pytest.ini_options]` with `testpaths = ["tests"]` and integration marker
- `[tool.ruff]` with `target-version = "py312"`, `line-length = 100`
- `[tool.ruff.lint]` with `select = ["E", "F", "I", "N", "W", "UP", "B", "SIM"]`

Then create the virtual environment and install dependencies:
- Run `py -3.12 -m venv .venv` to create venv with Python 3.12 explicitly
- Activate and install: `.venv/Scripts/pip install -r requirements-dev.txt`
- Verify: `.venv/Scripts/python --version` shows 3.12.x
  </action>
  <verify>
Run `.venv/Scripts/python -c "import openai; import azure.identity; import azure.monitor.query; import rich; print('All imports OK')"` -- should print "All imports OK".
Run `.venv/Scripts/python -m pytest --collect-only` -- should discover test directory (0 tests collected is fine at this point).
  </verify>
  <done>src/__init__.py, tests/__init__.py, requirements.txt, requirements-dev.txt, pyproject.toml all exist. Virtual environment created with Python 3.12. All Phase 1 dependencies installed and importable. pytest discovers the tests/ directory.</done>
</task>

<task type="auto">
  <name>Task 2: Implement config module with layered validation and mock fixtures</name>
  <files>src/config.py, src/__main__.py, tests/conftest.py, tests/test_config.py, tests/fixtures/chat_completion.json, tests/fixtures/tool_call_response.json, tests/fixtures/content_filter_error.json</files>
  <action>
**src/config.py** -- implement following the patterns from 01-RESEARCH.md exactly, with these locked decisions:

1. `Settings` dataclass with fields for: azure_openai_endpoint, azure_openai_api_key, azure_openai_chat_deployment (default "gpt-4o"), azure_openai_api_version (default "2024-10-21"), sentinel_workspace_id, azure_tenant_id, azure_client_id, azure_client_secret.

2. `REQUIRED_VARS` dict: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, SENTINEL_WORKSPACE_ID with descriptions.

3. `OPTIONAL_VARS` dict: AZURE_OPENAI_EMBEDDING_DEPLOYMENT ("Phase 4"), CHROMADB_PATH ("Phase 4") with phase annotations.

4. `load_settings() -> Settings`: Load from .env via python-dotenv load_dotenv(), return populated Settings.

5. `validate_env_vars() -> tuple[list[str], list[str]]`: Check ALL required vars, return (passed, failed) lists. Must show ALL missing vars at once (not fail-fast). Per user decision.

6. `test_openai_connectivity(settings: Settings) -> tuple[bool, str]`: Use AzureOpenAI client to send "Hello, respond with OK." with max_tokens=10. Detect content filter errors specifically:
   - Catch `openai.BadRequestError` where `e.code == "content_filter"` -> return message "Content filter modification pending -- approval required before security queries work"
   - Check `finish_reason == "content_filter"` on response -> same message
   - Catch `openai.AuthenticationError` -> "Azure OpenAI authentication failed -- check API key"
   - Catch `openai.APIConnectionError` -> "Azure OpenAI connection failed -- check endpoint URL"
   - Catch `openai.APIError` -> generic message with e.message

7. `test_sentinel_connectivity(settings: Settings) -> tuple[bool, str]`: Use DefaultAzureCredential + LogsQueryClient to run `SecurityIncident | take 1` with timespan=timedelta(days=1). Handle SUCCESS, PARTIAL (both return True), and errors. Detect auth errors ("run 'az login'"), resource not found ("check SENTINEL_WORKSPACE_ID"), and truncate generic error messages to 200 chars.

8. `validate_and_display()`: Orchestrate the two-layer validation:
   - Layer 1: Call validate_env_vars(). Display results.
   - If any required vars failed: show ALL missing vars, skip connectivity checks, display table with env var results only, exit with code 1.
   - Layer 2 (only if Layer 1 passes): Call load_settings(), then test_openai_connectivity() and test_sentinel_connectivity().
   - Display a rich Table with columns: Check, Status, Details. Rows for each env var check and each connectivity check. Use green checkmark for pass, red X for fail. Use rich Console for output.
   - Exit with code 0 if all pass, code 1 if any fail.

**src/__main__.py**: Enable `python -m src` execution. Route to config validation:
```python
from src.config import validate_and_display
validate_and_display()
```
Also support `python -m src.config` by checking sys.argv or module name.

**tests/fixtures/chat_completion.json**: Use the exact JSON from 01-RESEARCH.md (mock successful chat completion with security-related content, finish_reason "stop").

**tests/fixtures/tool_call_response.json**: Use the exact JSON from 01-RESEARCH.md (mock tool call with query_sentinel_incidents function).

**tests/fixtures/content_filter_error.json**: Use the exact JSON from 01-RESEARCH.md (mock content filter rejection with violence: medium).

**tests/conftest.py**: Create shared fixtures:
- `fixtures_dir` fixture returning Path to tests/fixtures/
- `chat_completion_fixture` loading chat_completion.json
- `tool_call_fixture` loading tool_call_response.json
- `content_filter_error_fixture` loading content_filter_error.json
- `mock_env_vars` fixture that sets required env vars to test values using monkeypatch
- `clean_env` fixture that clears all AZURE_* and SENTINEL_* env vars using monkeypatch

**tests/test_config.py**: Write unit tests covering:
- `test_validate_env_vars_all_present`: Set all required vars, verify all pass
- `test_validate_env_vars_all_missing`: Clear all vars, verify all fail
- `test_validate_env_vars_partial`: Set some vars, verify correct pass/fail split
- `test_load_settings_from_env`: Set vars, verify Settings fields populated correctly
- `test_load_settings_defaults`: Verify default values for chat_deployment and api_version
- `test_openai_connectivity_content_filter_input`: Mock AzureOpenAI to raise BadRequestError with code="content_filter", verify specific message returned
- `test_openai_connectivity_content_filter_output`: Mock AzureOpenAI to return response with finish_reason="content_filter", verify specific message
- `test_openai_connectivity_success`: Mock AzureOpenAI to return successful response, verify True and "Azure OpenAI connected"
- `test_openai_connectivity_auth_error`: Mock AzureOpenAI to raise AuthenticationError, verify specific message
- `test_sentinel_connectivity_success`: Mock LogsQueryClient to return SUCCESS status, verify True
- `test_sentinel_connectivity_auth_error`: Mock with auth error string, verify "az login" message

Use `unittest.mock.patch` for mocking Azure SDK clients. Do NOT call real Azure endpoints in unit tests. Use the `@pytest.mark.integration` marker definition in pyproject.toml for any future tests that need live resources (but do not create integration tests in this plan).
  </action>
  <verify>
Run `.venv/Scripts/python -m pytest tests/test_config.py -v` -- all tests should pass.
Run `.venv/Scripts/python -m ruff check src/ tests/` -- no linting errors.
Run `.venv/Scripts/python -c "from src.config import Settings, load_settings, validate_env_vars, test_openai_connectivity, test_sentinel_connectivity; print('All exports OK')"` -- should print "All exports OK".
  </verify>
  <done>
src/config.py implements Settings dataclass, load_settings(), validate_env_vars() (shows all missing vars at once), test_openai_connectivity() (with content-filter-specific detection), test_sentinel_connectivity(), and validate_and_display() (rich Table output).
src/__main__.py routes to config validation.
All three mock fixture JSON files exist in tests/fixtures/.
tests/conftest.py provides shared fixtures for loading mocks and managing env vars.
tests/test_config.py has 11+ passing tests covering env var validation, settings loading, content filter detection, and connectivity error handling.
All tests pass with pytest. No ruff linting errors.
  </done>
</task>

</tasks>

<verification>
1. `.venv/Scripts/python -m pytest -v` -- all tests pass, pytest discovers tests/ directory
2. `.venv/Scripts/python -m ruff check src/ tests/` -- no linting errors
3. `.venv/Scripts/python -c "from src.config import Settings, load_settings, validate_env_vars; print('imports work')"` -- confirms module structure
4. Verify src/config.py contains content-filter-specific error handling (not generic errors)
5. Verify validate_env_vars() collects ALL failures before returning (not fail-fast)
6. After Plan 01-01 is complete and .env is populated: `.venv/Scripts/python -m src.config` shows summary table with pass/fail for env vars and connectivity
</verification>

<success_criteria>
- src/__init__.py, src/__main__.py, src/config.py exist and are importable
- tests/__init__.py, tests/conftest.py, tests/test_config.py exist
- tests/fixtures/ has chat_completion.json, tool_call_response.json, content_filter_error.json
- requirements.txt, requirements-dev.txt, pyproject.toml exist with correct content
- .venv created with Python 3.12, all dependencies installed
- All pytest tests pass (11+ tests)
- No ruff linting errors
- python -m src.config runs without import errors (may fail on connectivity if .env not populated yet -- that is expected)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
