---
phase: 03-ai-orchestration-integration
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/openai_client.py
  - src/main.py
  - src/__main__.py
  - src/config.py
  - tests/test_openai_client.py
  - tests/test_main.py
  - tests/conftest.py
  - tests/fixtures/tool_call_response.json
autonomous: false
requirements: [ORCH-01, ORCH-02, ORCH-03]
user_setup: []

must_haves:
  truths:
    - "User can type a natural language question and receive a synthesized answer grounded in Sentinel tool results"
    - "User can have a multi-turn conversation where prior context is remembered (e.g., 'tell me more about that incident')"
    - "Conversation history is trimmed at 30 turns with a warning to the user before trimming"
    - "Tool loop terminates after MAX_TOOL_ROUNDS (5) and never enters an infinite loop"
    - "Chatbot response includes footnote-style reasoning showing which tools were used"
    - "/clear preserves a summary of the conversation and /quit exits cleanly"
    - "Tool execution shows simple text status messages (e.g., 'Querying incidents...')"
  artifacts:
    - path: "src/openai_client.py"
      provides: "ChatSession with tool loop and conversation management"
      contains: "class ChatSession"
    - path: "src/main.py"
      provides: "CLI chat loop entry point"
      contains: "def run_chat"
    - path: "src/__main__.py"
      provides: "Updated module entry point routing to run_chat"
      contains: "run_chat"
    - path: "src/config.py"
      provides: "Extended Settings with max_tool_rounds and max_turns"
      contains: "max_tool_rounds"
    - path: "tests/test_openai_client.py"
      provides: "ChatSession tests with mocked OpenAI client"
    - path: "tests/test_main.py"
      provides: "CLI loop tests"
  key_links:
    - from: "src/openai_client.py"
      to: "src/tools.py"
      via: "ChatSession passes SENTINEL_TOOLS to chat.completions.create"
      pattern: "SENTINEL_TOOLS"
    - from: "src/openai_client.py"
      to: "src/tool_handlers.py"
      via: "ChatSession calls ToolDispatcher.dispatch() for each tool_call"
      pattern: "self\\._dispatcher\\.dispatch"
    - from: "src/openai_client.py"
      to: "src/prompts.py"
      via: "ChatSession uses SYSTEM_PROMPT as first message"
      pattern: "SYSTEM_PROMPT"
    - from: "src/main.py"
      to: "src/openai_client.py"
      via: "run_chat creates ChatSession and calls send_message in loop"
      pattern: "ChatSession|send_message"
    - from: "src/main.py"
      to: "src/config.py"
      via: "run_chat loads Settings for client initialization"
      pattern: "load_settings|Settings"
---

<objective>
Build the ChatSession (OpenAI client + tool loop + conversation management) and CLI chat loop that ties everything together into a working chatbot.

Purpose: This is the integration plan that connects all Phase 3 components. ChatSession wraps the AzureOpenAI client, manages conversation history with turn-based trimming, runs the agentic tool loop, and tracks tool usage for transparency reporting. The CLI loop provides the user-facing /clear, /quit, /exit commands and prints responses.

Output: A working end-to-end chatbot: `python -m src` starts the chat loop, user types questions, chatbot queries Sentinel via tools, and returns grounded answers with reasoning footnotes.
</objective>

<execution_context>
@C:/Users/AlexSandstrom/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/AlexSandstrom/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ai-orchestration-integration/03-CONTEXT.md
@.planning/phases/03-ai-orchestration-integration/03-RESEARCH.md
@.planning/phases/03-ai-orchestration-integration/03-01-SUMMARY.md
@src/sentinel_client.py
@src/models.py
@src/config.py
@src/tools.py
@src/tool_handlers.py
@src/prompts.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ChatSession class with tool loop and conversation management</name>
  <files>src/openai_client.py, src/config.py</files>
  <action>
**src/config.py** -- Add two fields to the Settings dataclass:
- `max_tool_rounds: int = 5` -- configurable but not user-facing (per user decision)
- `max_turns: int = 30` -- conversation memory window (per user decision)

These should NOT be loaded from environment variables (they are internal tuning knobs). Just add them as dataclass fields with defaults.

**src/openai_client.py** -- Create ChatSession class that combines the OpenAI client, tool loop, and conversation management in ONE class (per approved architecture -- no separate orchestrator or conversation manager for this POC).

```python
class ChatSession:
    """Manages a chat session with Azure OpenAI including tool calling and conversation history.

    Combines:
    - AzureOpenAI client wrapper
    - Agentic tool-calling loop (max MAX_TOOL_ROUNDS iterations)
    - Conversation history with turn-based trimming (max MAX_TURNS)
    - Tool usage tracking for transparency reporting
    """

    def __init__(self, settings: Settings, *, client=None, sentinel_client=None):
        """Initialize ChatSession.

        Args:
            settings: Application settings.
            client: Optional AzureOpenAI client (for test injection).
            sentinel_client: Optional SentinelClient (for test injection).
        """
```

Key implementation details:

1. **Constructor**: Accept optional `client` (AzureOpenAI) and `sentinel_client` (SentinelClient) parameters for test injection (matching Phase 2 pattern). If not provided, create real clients from settings. Initialize ToolDispatcher with the sentinel_client. Store settings.max_tool_rounds and settings.max_turns. Initialize `self._messages: list` as empty (does NOT include system prompt -- that's prepended on each API call). Initialize `self._turn_count: int = 0`.

2. **send_message(user_input: str) -> str**: The main method. Flow:
   a. Append `{"role": "user", "content": user_input}` to self._messages
   b. Increment turn count
   c. Check if turn count exceeds max_turns. If so, trim oldest messages (remove pairs from the front) and print TOKEN_WARNING to stderr (using print(..., file=sys.stderr) so it doesn't mix with response output).
   d. Build full message list: `[{"role": "system", "content": SYSTEM_PROMPT}] + self._messages`
   e. Enter tool loop (max_tool_rounds iterations):
      - Call `self._client.chat.completions.create(model=self._model, messages=full_messages, tools=SENTINEL_TOOLS, tool_choice="auto")`
      - Get response_message = response.choices[0].message
      - Append response_message to self._messages (CRITICAL: must append assistant message BEFORE tool results per research)
      - If response_message.tool_calls is None or empty: break (we have the final answer)
      - For each tool_call in response_message.tool_calls:
        - Print status message to stderr: `print(dispatcher.get_status_message(tool_name), file=sys.stderr)`
        - Parse arguments: `json.loads(tool_call.function.arguments)`
        - Execute: `result = self._dispatcher.dispatch(tool_name, parsed_args)`
        - Append tool result to self._messages: `{"tool_call_id": tool_call.id, "role": "tool", "name": tool_name, "content": json.dumps(result)}`
        - Track in tool_log: `{"tool": tool_name, "args": parsed_args, "result_preview": _summarize_result(result)}`
      - After processing all tool_calls in this round, update full_messages for next iteration
   f. After loop: If the last message in self._messages is from assistant and has content, that's the answer. If we hit max_tool_rounds without a text response, append MAX_ROUNDS_MESSAGE and synthesize what we have.
   g. Return the final response content as a string.

3. **_trim_messages()**: Remove oldest user+assistant message pairs from self._messages until len is under max_turns * 2. Be careful to preserve tool call sequences -- don't split an assistant message with tool_calls from its corresponding tool result messages. Simple approach: remove messages from the front, but skip if removing would orphan a tool result (check role). Alternatively, remove in chunks: find the first "user" message and remove everything from start to the next "user" message.

4. **clear() -> str**: Per user decision, /clear should preserve a summary. Implementation:
   - If conversation is empty, return "Nothing to clear."
   - Build a summary request: send the current conversation to the LLM with CLEAR_SUMMARY_TEMPLATE as the user message (use a separate one-shot call, not the main message history)
   - Reset self._messages to contain only: `{"role": "system", "content": f"Previous session summary: {summary}"}` -- wait, system prompt is prepended separately. Instead, reset self._messages to `[{"role": "assistant", "content": f"Previous session context: {summary}"}]` so the summary carries forward as prior context.
   - Reset self._turn_count to 0
   - Return the summary text so the CLI can display it

5. **get_history_length() -> int**: Return current turn count (useful for testing/debugging).

6. **_summarize_result(result: dict) -> str**: Static/helper method. Create a brief text summary of a tool result for the tool_log. For QueryResult-shaped dicts: "{metadata.total} results, {query_ms}ms". For error dicts: "Error: {message}".

Important constraints from research:
- MUST append response_message (which contains tool_calls) BEFORE appending tool results
- Use `role: "tool"` NOT `role: "function"` (deprecated)
- Include `tool_call_id` on every tool result message
- Still pass `tools=SENTINEL_TOOLS` on subsequent loop iterations (model may call more tools)
- Do NOT set `parallel_tool_calls` parameter
- Do NOT use `strict: true` (already handled in tools.py, but don't override here)
  </action>
  <verify>
Run `python -c "from src.openai_client import ChatSession; from src.config import Settings; s = Settings(); print('max_tool_rounds:', s.max_tool_rounds, 'max_turns:', s.max_turns)"` -- should print max_tool_rounds: 5, max_turns: 30.
Run `python -c "from src.openai_client import ChatSession; print('ChatSession imported OK')"` -- imports without error.
  </verify>
  <done>ChatSession class exists with send_message(), clear(), and conversation history management. Settings has max_tool_rounds=5 and max_turns=30. Tool loop processes tool_calls correctly with proper message ordering. Turn-based trimming works at 30-turn boundary.</done>
</task>

<task type="auto">
  <name>Task 2: CLI chat loop, tests, and fixture updates</name>
  <files>src/main.py, src/__main__.py, tests/test_openai_client.py, tests/test_main.py, tests/conftest.py, tests/fixtures/tool_call_response.json</files>
  <action>
**src/main.py** -- Create the CLI chat loop:

```python
def run_chat() -> None:
    """Main CLI chat loop. Entry point for the chatbot."""
```

Implementation:
1. Load settings via `load_settings()`, validate with `validate_env_vars()` -- if validation fails, print error and exit
2. Create SentinelClient and ChatSession (pass settings to both)
3. Print welcome banner to stderr: brief intro explaining capabilities (query incidents, alerts, trends, entities) and available commands (/clear, /quit, /exit). Keep it concise -- 3-4 lines max.
4. Enter input loop:
   - Prompt: `"\nYou: "` (use `input()` -- rich prompt comes in Phase 5)
   - Handle KeyboardInterrupt (Ctrl+C): print goodbye, break
   - Handle EOFError: break
   - Strip input, skip empty
   - `/quit` or `/exit`: print goodbye, break
   - `/clear`: call session.clear(), print the summary and a "Conversation cleared" message, continue
   - Otherwise: call `response = session.send_message(user_input)`, print `f"\nAssistant: {response}"`
   - Wrap the send_message call in try/except for openai errors (APIError, AuthenticationError, etc.) -- print friendly error message, do NOT crash the loop

**src/__main__.py** -- Update to route to run_chat instead of validate_and_display:
```python
from src.main import run_chat
run_chat()
```

**tests/fixtures/tool_call_response.json** -- Update the existing fixture to match the actual tool names from src/tools.py. Change "query_sentinel_incidents" to "query_incidents" and update the arguments to match the actual schema: `{"time_window": "last_24h", "min_severity": "High"}`.

**tests/conftest.py** -- Add new fixtures for Phase 3 testing:
- `mock_settings` fixture: returns a Settings instance with test values (use mock_env_vars values plus max_tool_rounds=5, max_turns=30)
- `mock_openai_client` fixture: returns a MagicMock that simulates AzureOpenAI client behavior. Configure it so `client.chat.completions.create()` returns a mock response with choices[0].message having content="Test response" and no tool_calls.
- `mock_sentinel_client` fixture: returns a MagicMock of SentinelClient

Keep existing fixtures unchanged.

**tests/test_openai_client.py** -- Test ChatSession thoroughly:

1. **test_send_message_simple**: Mock OpenAI client to return a text response (no tool calls). Verify send_message returns the content. Verify conversation history has 2 messages (user + assistant).

2. **test_send_message_with_tool_call**: Mock OpenAI client to:
   - First call: return response with tool_calls=[{name: "query_incidents", arguments: '{"time_window": "last_24h"}'}]
   - Second call: return response with content="Here are your incidents..." and no tool_calls
   Mock ToolDispatcher.dispatch to return a known result dict.
   Verify: final response is "Here are your incidents...", tool was dispatched with correct args.

3. **test_tool_loop_max_rounds**: Mock OpenAI client to always return tool_calls (never a text response). Verify the loop terminates after max_tool_rounds iterations and returns a message containing MAX_ROUNDS_MESSAGE or similar.

4. **test_conversation_history_preserved**: Send two messages. Verify the second API call includes both the first user message, first assistant response, and the second user message in the messages list.

5. **test_turn_trimming**: Create ChatSession with max_turns=2. Send 3 messages. Verify oldest messages are trimmed from history.

6. **test_clear_with_summary**: Mock the summary LLM call to return "User discussed incidents." Call clear(). Verify messages are reset and summary is preserved. Verify turn count is 0.

7. **test_clear_empty_conversation**: Call clear() on fresh session. Verify returns "Nothing to clear" or similar.

8. **test_parallel_tool_calls**: Mock OpenAI to return response with 2 tool_calls in one response. Verify both are dispatched and results appended correctly.

9. **test_get_history_length**: Send a message, verify get_history_length returns 1.

For all tests, use the mock injection pattern: `ChatSession(settings, client=mock_client, sentinel_client=mock_sentinel_client)`. Do NOT make real API calls.

**tests/test_main.py** -- Test the CLI loop:

1. **test_quit_command**: Mock input to return "/quit". Verify run_chat exits cleanly.

2. **test_exit_command**: Mock input to return "/exit". Verify run_chat exits cleanly.

3. **test_clear_command**: Mock input sequence: ["/clear", "/quit"]. Mock ChatSession.clear to return "Summary text". Verify clear was called.

4. **test_send_message**: Mock input sequence: ["hello", "/quit"]. Mock ChatSession.send_message to return "Hi there!". Verify send_message was called with "hello".

5. **test_keyboard_interrupt**: Mock input to raise KeyboardInterrupt. Verify run_chat exits cleanly without traceback.

Use `unittest.mock.patch` to mock `builtins.input` and patch ChatSession/SentinelClient constructors. Use `monkeypatch` for env vars via the mock_env_vars fixture.
  </action>
  <verify>
Run `pytest tests/test_openai_client.py tests/test_main.py -v` -- all tests pass.
Run `pytest` -- full test suite passes (including updated fixture).
Run `ruff check src/openai_client.py src/main.py src/__main__.py` -- no lint errors.
Run `python -c "from src.main import run_chat; print('CLI entry point OK')"` -- imports cleanly.
  </verify>
  <done>ChatSession fully tested with tool loop, conversation trimming, clear functionality, and parallel tool calls. CLI loop handles /clear, /quit, /exit, and KeyboardInterrupt. Updated fixture matches actual tool schemas. Full test suite green.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: End-to-end verification of chatbot against live Sentinel</name>
  <files></files>
  <action>Verify the complete chatbot works end-to-end against live Azure OpenAI and Sentinel APIs.</action>
  <what-built>Complete end-to-end Sentinel chatbot with natural language querying, tool calling, multi-turn conversation, and grounded responses. Running `python -m src` starts the chat loop.</what-built>
  <how-to-verify>
1. Activate venv: `source .venv/bin/activate` (or `.venv\Scripts\activate` on Windows)
2. Run: `python -m src`
3. Verify welcome message appears with capabilities overview
4. Test basic query: Type "show me high severity incidents from the last 24 hours"
   - Should see "Querying incidents..." status message
   - Should get a response with incident data in a readable format
   - Should see footnote section listing tools used and data retrieved
5. Test multi-turn: Type "tell me more about the first one" or "tell me more about [1]"
   - Should see "Looking up incident details..." status message
   - Should get detailed incident info
6. Test empty results: Type "show me high severity incidents from the last hour"
   - If no results, should suggest broadening severity or time range
7. Test /clear: Type "/clear"
   - Should see conversation summary
   - Should see "Conversation cleared" message
   - Ask another question to verify the chatbot still works after clear
8. Test /quit: Type "/quit" -- should exit cleanly
9. Run `pytest -v` to confirm all tests pass
  </how-to-verify>
  <verify>All manual verification steps above pass.</verify>
  <done>Chatbot successfully queries live Sentinel data via natural language, maintains multi-turn context, shows reasoning transparency, and handles /clear and /quit commands.</done>
  <resume-signal>Type "approved" or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. `python -m src` starts the chat loop (not config validation)
2. Natural language question triggers correct tool and returns grounded answer
3. Multi-turn conversation preserves context
4. Footnote-style tool usage reporting appears in responses
5. /clear preserves summary, /quit exits cleanly
6. `pytest` -- full suite green
7. `ruff check src/` -- no lint errors
</verification>

<success_criteria>
- ChatSession manages conversation history with 30-turn trimming
- Tool loop runs up to 5 rounds, correctly dispatching tools and appending results
- Responses include footnote-style reasoning transparency (tools used, data found)
- /clear generates and preserves conversation summary
- CLI handles /quit, /exit, /clear, KeyboardInterrupt gracefully
- All tests pass including new ChatSession and CLI tests
- End-to-end: user asks natural language question, gets grounded answer from live Sentinel data
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-orchestration-integration/03-02-SUMMARY.md`
</output>
