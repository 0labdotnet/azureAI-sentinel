---
phase: 04-knowledge-base
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/config.py
  - src/vector_store.py
  - src/knowledge/__init__.py
  - src/knowledge/seed_incidents.py
  - src/knowledge/playbooks.py
  - src/mitre.py
  - tests/test_vector_store.py
  - tests/test_mitre.py
autonomous: true
requirements: [KB-01, KB-02, KB-03]

must_haves:
  truths:
    - "VectorStore creates two ChromaDB collections (incidents, playbooks) with cosine distance"
    - "Synthetic seed data contains 15-25 incidents covering common attack types"
    - "Five hand-written playbooks cover phishing, brute force, malware, suspicious sign-in, and data exfiltration with actionable investigation steps and KQL queries"
    - "MITRE ATT&CK fetcher downloads enterprise techniques from GitHub, filters to curated 25 techniques, and caches the result locally"
    - "VectorStore search methods return top 3 results with low-confidence flagging at cosine distance threshold 0.35"
    - "Config Settings includes azure_openai_embedding_deployment and chromadb_path fields"
  artifacts:
    - path: "src/vector_store.py"
      provides: "VectorStore class with upsert_incidents, upsert_playbooks, search_similar_incidents, search_playbooks methods"
      min_lines: 80
    - path: "src/knowledge/seed_incidents.py"
      provides: "15-25 synthetic incident dicts with title, severity, description, mitre_techniques, entities"
      min_lines: 100
    - path: "src/knowledge/playbooks.py"
      provides: "5 detailed playbook dicts with investigation steps, indicators, containment, escalation, KQL queries"
      min_lines: 200
    - path: "src/mitre.py"
      provides: "fetch_mitre_techniques() function returning curated ATT&CK techniques with caching"
      min_lines: 50
    - path: "tests/test_vector_store.py"
      provides: "VectorStore unit tests using EphemeralClient"
      min_lines: 50
    - path: "tests/test_mitre.py"
      provides: "MITRE fetcher tests with mocked HTTP"
      min_lines: 30
  key_links:
    - from: "src/vector_store.py"
      to: "src/config.py"
      via: "Settings dataclass fields for embedding deployment and chromadb path"
      pattern: "settings\\.azure_openai_embedding_deployment|settings\\.chromadb_path"
    - from: "src/vector_store.py"
      to: "chromadb"
      via: "PersistentClient and OpenAIEmbeddingFunction"
      pattern: "chromadb\\.PersistentClient|OpenAIEmbeddingFunction"
    - from: "src/mitre.py"
      to: "stix2"
      via: "MemoryStore and Filter for ATT&CK data"
      pattern: "stix2\\.MemoryStore|stix2\\.Filter"
---

<objective>
Build the ChromaDB-backed VectorStore class, seed knowledge base content (synthetic incidents + hand-written playbooks), MITRE ATT&CK data fetcher, and all unit tests.

Purpose: This is the data foundation for the knowledge base. Without the VectorStore, seed data, and MITRE fetcher, the tool integration in Plan 02 has nothing to wire into. This plan creates all the "data layer" artifacts that Plan 02 will integrate into the existing tool loop.

Output: VectorStore class, 15-25 synthetic incidents, 5 detailed playbooks, MITRE fetcher with caching, config updates, and tests for all new modules.
</objective>

<execution_context>
@C:/Users/AlexSandstrom/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/AlexSandstrom/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-knowledge-base/04-CONTEXT.md
@.planning/phases/04-knowledge-base/04-RESEARCH.md
@src/config.py
@src/models.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: VectorStore class, config updates, and dependencies</name>
  <files>
    requirements.txt
    src/config.py
    src/vector_store.py
    src/knowledge/__init__.py
  </files>
  <action>
1. Update requirements.txt: add `chromadb>=1.5.0`, `stix2>=3.0.2`, `requests>=2.31.0`. Run `pip install -r requirements.txt` to install.

2. Update src/config.py Settings dataclass: add two new fields:
   - `azure_openai_embedding_deployment: str = "text-embedding-3-large"`
   - `chromadb_path: str = "./chroma_db"`
   Update `load_settings()` to read these from env vars `AZURE_OPENAI_EMBEDDING_DEPLOYMENT` and `CHROMADB_PATH` with the above defaults. Move these two entries out of OPTIONAL_VARS comments since they are now active.

3. Create src/knowledge/__init__.py as an empty package init file.

4. Create src/vector_store.py with the VectorStore class. Follow the exact pattern from 04-RESEARCH.md Pattern 2:
   - Constructor accepts `Settings` and optional `client: chromadb.ClientAPI | None = None` for test injection (matching the project's DI pattern from SentinelClient and ChatSession).
   - Create `OpenAIEmbeddingFunction` configured for Azure: `api_key`, `api_base` (from settings.azure_openai_endpoint), `api_type="azure"`, `api_version`, `model_name` and `deployment_id` both set to settings.azure_openai_embedding_deployment, `dimensions=1024`.
   - Use `PersistentClient(path=settings.chromadb_path)` when no client injected.
   - Create two collections via `get_or_create_collection()`: "incidents" and "playbooks", both with `configuration={"hnsw": {"space": "cosine"}}` and the embedding function.
   - Implement methods:
     - `upsert_incidents(incidents: list[dict]) -> int` -- upserts docs with ids, documents, metadatas
     - `upsert_playbooks(chunks: list[dict]) -> int` -- upserts playbook chunks
     - `search_similar_incidents(query: str, n_results: int = 3) -> dict` -- queries incidents collection
     - `search_playbooks(query: str, n_results: int = 3) -> dict` -- queries playbooks collection
     - `_format_results(results: dict, result_type: str, threshold: float = 0.35) -> dict` -- formats ChromaDB results with confidence flagging. Returns `{"type": result_type, "results": [...], "low_confidence_warning": bool, "total": int}`. Each result item has "document", "metadata", "confidence" ("normal" or "low"). Per user decision: do NOT include distance/score in the output.
     - `get_collection_counts() -> dict` -- returns `{"incidents": N, "playbooks": N}`
   - IMPORTANT: Use cosine distance threshold of 0.35 for low-confidence flagging. Below 0.35 = normal confidence, above 0.35 = low confidence. `low_confidence_warning` is True only when ALL results are low confidence.

CRITICAL: The `OpenAIEmbeddingFunction` import path is `from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction`. Do NOT use deprecated paths.

CRITICAL: Use `configuration={"hnsw": {"space": "cosine"}}` (new API), NOT `metadata={"hnsw:space": "cosine"}` (deprecated).
  </action>
  <verify>
Run `python -c "from src.vector_store import VectorStore; print('VectorStore imported OK')"` and `python -c "from src.config import load_settings; s = load_settings(); print(s.azure_openai_embedding_deployment, s.chromadb_path)"` to verify imports and config. Run `pytest tests/ -x` to confirm no regressions.
  </verify>
  <done>
VectorStore class exists with all 6 public methods (upsert_incidents, upsert_playbooks, search_similar_incidents, search_playbooks, get_collection_counts, _format_results). Settings has embedding_deployment and chromadb_path fields. All existing tests still pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Seed data, playbooks, MITRE fetcher, and tests</name>
  <files>
    src/knowledge/seed_incidents.py
    src/knowledge/playbooks.py
    src/mitre.py
    tests/test_vector_store.py
    tests/test_mitre.py
  </files>
  <action>
1. Create src/knowledge/seed_incidents.py with SEED_INCIDENTS list of 20 synthetic incident dicts. Each dict has:
   - `id`: "synthetic-{index}" (string)
   - `title`: descriptive incident title (e.g., "Phishing email with malicious attachment detected")
   - `severity`: one of "High", "Medium", "Low", "Informational"
   - `status`: one of "New", "Active", "Closed"
   - `description`: 2-3 sentence natural language description of the incident
   - `mitre_techniques`: comma-separated technique IDs (e.g., "T1566,T1204")
   - `entities`: affected entities as a string (e.g., "user: jsmith@contoso.com, host: WS-PC01")
   - `source`: "synthetic"
   Cover these attack types: phishing (3), brute force (3), malware/ransomware (3), suspicious sign-in (3), data exfiltration (2), privilege escalation (2), lateral movement (2), denial of service (1), credential theft (1). Use realistic-sounding but obviously fake data (contoso.com domain, generic hostnames).
   Also provide a `build_incident_document(incident: dict) -> str` function that converts an incident dict into a natural-language document for embedding, and a `build_incident_metadata(incident: dict) -> dict` function that extracts metadata fields (incident_number=0 for synthetic, title, severity, status, source, mitre_techniques, created_date as today's ISO date string).

2. Create src/knowledge/playbooks.py with PLAYBOOKS list of 5 detailed playbook dicts. Each playbook covers one incident type: phishing, brute force, malware, suspicious sign-in, data exfiltration. Each playbook dict has:
   - `playbook_id`: e.g., "phishing-01"
   - `incident_type`: e.g., "Phishing"
   - `mitre_techniques`: comma-separated technique IDs
   - `sections`: list of section dicts, each with:
     - `section`: one of "investigation", "indicators", "containment", "escalation"
     - `content`: detailed actionable text (3-8 sentences per section). Investigation sections MUST include at least one copy-pasteable KQL query. Content should feel actionable for a SOC analyst, not abstract. Include the playbook title and incident type in each section's content for self-contained context.
   Also provide a `build_playbook_chunks(playbook: dict) -> list[dict]` function that converts a playbook into a list of chunk dicts ready for VectorStore.upsert_playbooks(). Each chunk has: `id` (e.g., "phishing-01-investigation-0"), `document` (the section content prefixed with "Playbook: {incident_type} - {section}\n\n"), `metadata` (playbook_id, incident_type, mitre_techniques, section, chunk_index, source="hand-written").

3. Create src/mitre.py with:
   - A curated list CURATED_TECHNIQUE_IDS containing 25 ATT&CK technique IDs: T1566, T1078, T1190, T1059, T1204, T1136, T1053, T1098, T1548, T1134, T1562, T1070, T1110, T1003, T1558, T1021, T1570, T1005, T1567, T1041, T1087, T1069, T1486, T1489, T1071 (from RESEARCH.md discretionary recommendations).
   - `fetch_mitre_techniques(cache_dir: str | None = None) -> list[dict]`:
     - If cache_dir is provided, check for `{cache_dir}/enterprise-attack.json`. If it exists and is less than 24 hours old, load from cache. Otherwise download from `https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/enterprise-attack/enterprise-attack.json` (with timeout=60) and save to cache.
     - Parse using `stix2.MemoryStore`, filter for `type=attack-pattern` and `x_mitre_is_subtechnique=False`.
     - Extract technique_id (from external_references), name, description, and tactics (from kill_chain_phases).
     - Filter to only techniques whose technique_id is in CURATED_TECHNIQUE_IDS.
     - Return list of dicts with keys: technique_id, name, description, tactics.
     - On any exception (network error, parse error), log a warning and return an empty list (graceful degradation per CONTEXT.md discretion).

4. Create tests/test_vector_store.py:
   - Use `chromadb.EphemeralClient()` for test injection (per RESEARCH.md testing strategy).
   - Test upsert_incidents: add 3 incidents, verify count == 3, add same IDs again (idempotent), verify count still 3.
   - Test upsert_playbooks: add 2 chunks, verify count == 2.
   - Test search_similar_incidents: add incidents, query with related text, verify results returned with type="similar_incidents" and results list.
   - Test search_playbooks: add playbook chunks, query, verify results returned with type="playbooks".
   - Test _format_results low-confidence flagging: mock results with high distances (>0.35), verify low_confidence_warning is True.
   - Test get_collection_counts: verify returns correct counts for both collections.
   - IMPORTANT: Since EphemeralClient doesn't call Azure OpenAI for embeddings, you'll need to either mock the embedding function or pass a simple test embedding function. Use a lambda or simple class that returns fixed-length float lists (1024 dims) for test purposes. The simplest approach: create a mock embedding function class that implements `__call__` and returns deterministic embeddings based on text hash.

5. Create tests/test_mitre.py:
   - Mock `requests.get` to return a small STIX bundle with 3 technique objects (2 in curated list, 1 not).
   - Test fetch_mitre_techniques returns only the 2 curated techniques.
   - Test caching: call twice, verify only one HTTP request.
   - Test graceful failure: mock requests.get to raise ConnectionError, verify empty list returned.

Run `pytest tests/ -v` to verify all tests pass. Run `ruff check src/ tests/` for lint.
  </action>
  <verify>
Run `pytest tests/ -v` -- all tests pass including new test_vector_store.py and test_mitre.py. Run `ruff check src/ tests/` -- no lint errors. Run `python -c "from src.knowledge.seed_incidents import SEED_INCIDENTS; print(f'{len(SEED_INCIDENTS)} seed incidents')"` confirms 20 incidents. Run `python -c "from src.knowledge.playbooks import PLAYBOOKS; print(f'{len(PLAYBOOKS)} playbooks')"` confirms 5 playbooks.
  </verify>
  <done>
20 synthetic incidents exist covering 9 attack types with realistic descriptions and MITRE technique tags. 5 detailed playbooks exist for phishing, brute force, malware, suspicious sign-in, and data exfiltration -- each with investigation (including KQL), indicators, containment, and escalation sections. MITRE fetcher downloads and caches ATT&CK data, filtering to 25 curated techniques. All new tests pass alongside existing test suite (zero regressions).
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/ -v` -- all tests pass (existing + new)
2. `ruff check src/ tests/` -- no lint errors
3. `python -c "from src.vector_store import VectorStore"` -- imports successfully
4. `python -c "from src.knowledge.seed_incidents import SEED_INCIDENTS; print(len(SEED_INCIDENTS))"` -- prints 20
5. `python -c "from src.knowledge.playbooks import PLAYBOOKS, build_playbook_chunks; total = sum(len(build_playbook_chunks(p)) for p in PLAYBOOKS); print(f'{len(PLAYBOOKS)} playbooks, {total} chunks')"` -- prints 5 playbooks with chunk count
6. `python -c "from src.mitre import CURATED_TECHNIQUE_IDS; print(len(CURATED_TECHNIQUE_IDS))"` -- prints 25
</verification>

<success_criteria>
- VectorStore class with two collections, upsert, search, and confidence flagging works correctly
- 20 synthetic incidents covering 9 attack types
- 5 playbooks with 4 sections each, including KQL queries in investigation sections
- MITRE fetcher with 24-hour file caching and graceful failure handling
- All new and existing tests pass
- Settings dataclass has embedding_deployment and chromadb_path fields
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-base/04-01-SUMMARY.md`
</output>
