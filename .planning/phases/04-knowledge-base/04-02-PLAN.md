---
phase: 04-knowledge-base
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/tools.py
  - src/tool_handlers.py
  - src/prompts.py
  - src/openai_client.py
  - src/main.py
  - tests/test_tools.py
  - tests/test_tool_handlers.py
  - tests/test_main.py
  - tests/test_openai_client.py
autonomous: true
requirements: [KB-01, KB-02, KB-03]

must_haves:
  truths:
    - "User can ask for investigation guidance and receive MITRE ATT&CK-mapped recommendations from the knowledge base"
    - "User can ask 'have we seen this before?' and receive semantically matched historical incidents"
    - "User can ask about response procedures and receive playbook-based guidance"
    - "Knowledge base tools are included in the ChatSession tool loop alongside Sentinel tools"
    - "Chatbot auto-ingests seed data, live Sentinel incidents, playbooks, and MITRE techniques on startup"
    - "System prompt guides the LLM on when to use KB tools vs Sentinel tools"
    - "Search results present 'Similar past incidents' and 'Relevant playbooks' as distinct sections"
    - "Low-confidence matches produce a warning in the LLM's response"
  artifacts:
    - path: "src/tools.py"
      provides: "KB_TOOLS list with 3 new tool definitions (search_similar_incidents, search_playbooks, get_investigation_guidance)"
      contains: "KB_TOOLS"
    - path: "src/tool_handlers.py"
      provides: "ToolDispatcher updated with KB tool routing to VectorStore"
      contains: "search_similar_incidents"
    - path: "src/prompts.py"
      provides: "System prompt updated with KB tool usage guidance"
      contains: "knowledge base"
    - path: "src/openai_client.py"
      provides: "ChatSession passing combined Sentinel + KB tools to LLM"
      contains: "KB_TOOLS"
    - path: "src/main.py"
      provides: "Startup pipeline initializing VectorStore and ingesting data"
      contains: "VectorStore"
  key_links:
    - from: "src/tool_handlers.py"
      to: "src/vector_store.py"
      via: "ToolDispatcher routes KB tool calls to VectorStore methods"
      pattern: "_vector_store\\.search_similar_incidents|_vector_store\\.search_playbooks"
    - from: "src/openai_client.py"
      to: "src/tools.py"
      via: "ChatSession passes SENTINEL_TOOLS + KB_TOOLS to chat.completions.create()"
      pattern: "SENTINEL_TOOLS.*KB_TOOLS|KB_TOOLS"
    - from: "src/main.py"
      to: "src/vector_store.py"
      via: "Startup creates VectorStore and runs ingestion pipeline"
      pattern: "VectorStore|upsert_incidents|upsert_playbooks"
    - from: "src/main.py"
      to: "src/mitre.py"
      via: "Startup fetches MITRE techniques for enrichment"
      pattern: "fetch_mitre_techniques"
    - from: "src/prompts.py"
      to: "LLM behavior"
      via: "System prompt KB guidance section directs LLM to use KB tools"
      pattern: "search_similar_incidents|search_playbooks|get_investigation_guidance"
---

<objective>
Integrate the knowledge base into the existing chatbot tool loop: add KB tool definitions, wire ToolDispatcher to route KB calls to VectorStore, update system prompt with KB guidance, build the startup ingestion pipeline, and update all tests.

Purpose: Plan 01 built the data layer (VectorStore, seed data, playbooks, MITRE). This plan wires everything into the existing chatbot so users can actually ask questions about historical incidents, playbooks, and MITRE ATT&CK techniques through natural language. After this plan, all three KB requirements (KB-01, KB-02, KB-03) are fully satisfied end-to-end.

Output: Updated tools.py, tool_handlers.py, prompts.py, openai_client.py, main.py with KB integration, plus updated tests.
</objective>

<execution_context>
@C:/Users/AlexSandstrom/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/AlexSandstrom/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-knowledge-base/04-CONTEXT.md
@.planning/phases/04-knowledge-base/04-RESEARCH.md
@.planning/phases/04-knowledge-base/04-01-SUMMARY.md
@src/tools.py
@src/tool_handlers.py
@src/prompts.py
@src/openai_client.py
@src/main.py
@src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: KB tool definitions, dispatcher wiring, and system prompt update</name>
  <files>
    src/tools.py
    src/tool_handlers.py
    src/prompts.py
    src/openai_client.py
  </files>
  <action>
1. Update src/tools.py: Add KB_TOOLS list with 3 tool definitions following the exact pattern from 04-RESEARCH.md (search_similar_incidents, search_playbooks, get_investigation_guidance). Each tool has `type: "function"` with `name`, `description`, and `parameters` (single required `query` string param). Descriptions must guide the LLM on when to use each tool:
   - search_similar_incidents: "have we seen this before?", "similar attacks", "historical incidents like X"
   - search_playbooks: "how to investigate X", "response procedure for Y", "investigation guidance"
   - get_investigation_guidance: "MITRE techniques", "ATT&CK mappings", "what techniques are involved in X"
   Update `get_tool_names()` to include KB tool names: iterate over SENTINEL_TOOLS + KB_TOOLS.

2. Update src/tool_handlers.py:
   - Import VectorStore from src.vector_store.
   - Modify ToolDispatcher.__init__ to accept an optional `vector_store: VectorStore | None = None` parameter (for test injection and graceful degradation when KB is unavailable).
   - Store `self._vector_store = vector_store`.
   - Add KB tool handlers to `_dispatch_map` (only if vector_store is not None):
     - "search_similar_incidents" -> self._search_similar_incidents
     - "search_playbooks" -> self._search_playbooks
     - "get_investigation_guidance" -> self._get_investigation_guidance
   - Add status messages: "search_similar_incidents": "Searching historical incidents...", "search_playbooks": "Searching playbooks...", "get_investigation_guidance": "Looking up investigation guidance..."
   - Implement the 3 private handler methods:
     - `_search_similar_incidents(args)`: extract query string, call self._vector_store.search_similar_incidents(query), return result dict directly (already formatted by VectorStore).
     - `_search_playbooks(args)`: extract query string, call self._vector_store.search_playbooks(query), return result dict.
     - `_get_investigation_guidance(args)`: extract query string, search BOTH playbooks AND incidents for MITRE-related content. Call self._vector_store.search_playbooks(query, n_results=3) and self._vector_store.search_similar_incidents(query, n_results=3), combine into a single response dict: `{"type": "investigation_guidance", "playbook_results": playbook_result["results"], "incident_results": incident_result["results"], "low_confidence_warning": playbook_result["low_confidence_warning"] and incident_result["low_confidence_warning"]}`.
   - If vector_store is None and a KB tool is called, return `{"error": "Knowledge base is not available. Try restarting the chatbot."}`.

3. Update src/prompts.py SYSTEM_PROMPT: Add a new section after the existing "## Tool Usage Guidance" section. Insert KB tool guidance:
   ```
   - **Historical context** ("have we seen this before?", "similar incidents"): \
   Use search_similar_incidents to find past incidents with similar patterns.
   - **Investigation guidance** ("how do I investigate?", "response procedure"): \
   Use search_playbooks for step-by-step investigation playbooks with KQL queries.
   - **MITRE ATT&CK mapping** ("what techniques?", "ATT&CK mapping for this"): \
   Use get_investigation_guidance for technique-mapped recommendations \
   combining playbooks and historical context.
   ```
   Also add guidance about result presentation: "When presenting knowledge base results, separate them into 'Similar past incidents' and 'Relevant playbooks' sections. If results have a low_confidence_warning, inform the user that the matches may not be highly relevant and suggest refining their query."

4. Update src/openai_client.py:
   - Import KB_TOOLS from src.tools and VectorStore from src.vector_store.
   - Modify ChatSession.__init__ to accept optional `vector_store: VectorStore | None = None` parameter.
   - Pass vector_store to ToolDispatcher: `self._dispatcher = ToolDispatcher(self._sentinel_client, vector_store=vector_store)`.
   - Combine tools for the LLM: create `self._tools = SENTINEL_TOOLS + KB_TOOLS` if vector_store is not None, else `self._tools = SENTINEL_TOOLS`.
   - In send_message(), use `self._tools` instead of `SENTINEL_TOOLS` in the `tools=` parameter to `chat.completions.create()`.
  </action>
  <verify>
Run `python -c "from src.tools import SENTINEL_TOOLS, KB_TOOLS, get_tool_names; print(f'{len(SENTINEL_TOOLS)} sentinel + {len(KB_TOOLS)} kb = {len(get_tool_names())} total')"` -- prints "5 sentinel + 3 kb = 8 total". Run `pytest tests/ -x` to confirm no regressions (existing tests should still pass since VectorStore is optional).
  </verify>
  <done>
3 KB tool definitions added to tools.py. ToolDispatcher routes KB tools to VectorStore methods with graceful None handling. System prompt includes KB tool usage guidance and result presentation rules. ChatSession passes combined tool list to the LLM when VectorStore is provided. All existing tests still pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Startup ingestion pipeline, updated tests, and end-to-end wiring</name>
  <files>
    src/main.py
    tests/test_tools.py
    tests/test_tool_handlers.py
    tests/test_main.py
    tests/test_openai_client.py
  </files>
  <action>
1. Update src/main.py run_chat() to build the startup ingestion pipeline. After creating `sentinel_client` and before creating `session`, add:

   a. Create VectorStore:
      ```python
      from src.vector_store import VectorStore
      vector_store = VectorStore(settings)
      ```
      Wrap in try/except -- if VectorStore creation fails (ChromaDB or embedding function error), log warning to stderr, set vector_store = None, and continue without KB.

   b. Seed data ingestion (only if vector_store is not None):
      ```python
      from src.knowledge.seed_incidents import SEED_INCIDENTS, build_incident_document, build_incident_metadata
      from src.knowledge.playbooks import PLAYBOOKS, build_playbook_chunks
      ```
      - Build incident docs from SEED_INCIDENTS and upsert: create list of `{"id": inc["id"], "document": build_incident_document(inc), "metadata": build_incident_metadata(inc)}` for each incident, call `vector_store.upsert_incidents(...)`.
      - Build playbook chunks from all PLAYBOOKS: `[chunk for pb in PLAYBOOKS for chunk in build_playbook_chunks(pb)]`, call `vector_store.upsert_playbooks(...)`.

   c. Live Sentinel incident ingestion (only if vector_store is not None):
      - Call `sentinel_client.query_incidents(time_window="last_30d", min_severity="Informational", limit=100)` to get live incidents.
      - If result is a QueryResult (not QueryError), convert each incident to vector store format: id=f"incident-{inc.number}", document=build_incident_document with title/severity/status/description from the Incident dataclass, metadata with incident_number, title, severity, status, source="sentinel", mitre_techniques="" (empty -- live incidents don't have this), created_date from incident.created_time.
      - Upsert the live incidents. If Sentinel query fails, log warning and continue (graceful degradation).

   d. MITRE ATT&CK ingestion (only if vector_store is not None):
      ```python
      from src.mitre import fetch_mitre_techniques
      ```
      - Determine cache dir: `os.path.join(settings.chromadb_path, "mitre_cache")`, create dir if needed with `os.makedirs(cache_dir, exist_ok=True)`.
      - Call `fetch_mitre_techniques(cache_dir=cache_dir)`.
      - This is for enrichment context. The MITRE techniques don't need to be stored in ChromaDB separately -- they enrich the playbooks which already have MITRE technique IDs. However, log the count for transparency.

   e. Print startup summary to stderr: "Knowledge base loaded: {N} incidents, {M} playbooks" using `vector_store.get_collection_counts()`. If vector_store is None, print "Knowledge base unavailable -- running with Sentinel tools only."

   f. Pass vector_store to ChatSession: `session = ChatSession(settings, sentinel_client=sentinel_client, vector_store=vector_store)`.

   g. Update the welcome banner to mention knowledge base capabilities: add "Ask about historical incidents, playbooks, or MITRE ATT&CK techniques." to the banner text.

2. Update tests/test_tools.py: Add tests for KB_TOOLS structure -- verify 3 tools, correct names, each has required "query" parameter. Verify get_tool_names() returns 8 names total.

3. Update tests/test_tool_handlers.py: Add tests for KB tool dispatch:
   - Test dispatch with vector_store=None returns error dict for KB tools.
   - Test search_similar_incidents dispatch calls vector_store.search_similar_incidents with correct query.
   - Test search_playbooks dispatch calls vector_store.search_playbooks.
   - Test get_investigation_guidance dispatch calls both search methods and combines results.
   - Use MagicMock for VectorStore in these tests (matching existing mock patterns).

4. Update tests/test_main.py: Ensure run_chat tests still work. Since the startup pipeline now creates VectorStore, mock VectorStore in the test fixtures. Add test that verifies run_chat prints knowledge base status on startup.

5. Update tests/test_openai_client.py: Add test that ChatSession with vector_store=not-None includes KB_TOOLS in the tools parameter. Add test that ChatSession with vector_store=None only uses SENTINEL_TOOLS. Mock the chat.completions.create call and verify the `tools` parameter length (5 for Sentinel-only, 8 for Sentinel+KB).

Run `pytest tests/ -v` to verify all tests pass. Run `ruff check src/ tests/` for lint.
  </action>
  <verify>
Run `pytest tests/ -v` -- all tests pass (existing + new). Run `ruff check src/ tests/` -- no lint errors. Run `python -c "from src.tools import get_tool_names; print(get_tool_names())"` -- prints 8 tool names. Verify main.py imports compile: `python -c "import src.main"`.
  </verify>
  <done>
Startup pipeline initializes VectorStore, ingests seed incidents, playbooks, live Sentinel incidents (graceful on failure), and fetches MITRE ATT&CK data. ChatSession receives combined Sentinel+KB tools. Welcome banner mentions KB capabilities. All tests pass including new KB dispatch tests, tool count tests, and startup tests. The complete end-to-end flow works: user asks a KB question -> LLM selects KB tool -> ToolDispatcher routes to VectorStore -> results returned to LLM for synthesis.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/ -v` -- all tests pass
2. `ruff check src/ tests/` -- no lint errors
3. `python -c "from src.tools import get_tool_names; names = get_tool_names(); assert len(names) == 8; print(names)"` -- 8 tools total
4. `python -c "from src.tool_handlers import ToolDispatcher; print('ToolDispatcher supports KB tools')"` -- imports OK
5. `python -c "from src.prompts import SYSTEM_PROMPT; assert 'knowledge base' in SYSTEM_PROMPT.lower(); print('System prompt includes KB guidance')"` -- KB guidance present
6. Verify main.py startup flow compiles: `python -c "import ast; ast.parse(open('src/main.py').read()); print('main.py parses OK')"`
</verification>

<success_criteria>
- 3 KB tools (search_similar_incidents, search_playbooks, get_investigation_guidance) registered in tools.py
- ToolDispatcher routes KB tools to VectorStore with graceful None fallback
- System prompt includes KB tool usage guidance and result presentation rules
- ChatSession passes Sentinel+KB tools to LLM when VectorStore available
- Startup pipeline ingests seed data, live Sentinel incidents, playbooks, and MITRE techniques
- All tests pass (new KB tests + existing tests with zero regressions)
- Graceful degradation: if VectorStore unavailable, chatbot runs with Sentinel-only tools
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-base/04-02-SUMMARY.md`
</output>
